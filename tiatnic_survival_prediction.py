# -*- coding: utf-8 -*-
"""TIATNIC SURVIVAL PREDICTION.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BAk3yqNMqb-pC2jIEbx5pwWno22Kbodj
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

train_pd = pd.read_csv('train.csv')
test_pd = pd.read_csv('test.csv')
display (train_pd)

survived = train_pd[ 'Survived' ]
train_pd = train_pd.drop(['Survived'], axis=1)

display(train_pd)

train_idx = train_pd['PassengerId']
test_idx = test_pd['PassengerId']

combined_pd = pd.concat([train_pd, test_pd]).reset_index(drop=True)
display (combined_pd)

print('NULL VALUES:')
print(combined_pd.isnull().sum())

y = combined_pd ['Age']. value_counts()
x = y. index. values
plt.figure(figsize=(10,5))
plt.bar(x,y)
plt. title('All Ages')
plt. show()
class_age = combined_pd[['Pclass' ,
'Age' ]]
for i in range (1,4):
    plt. figure(figsize=(10,5))
    plt. title('Class ' + str(i))
    y = class_age[class_age['Pclass']==i]['Age'].value_counts()
    x = y. index.values
    plt.bar (x,y)
    plt.show()

medians = class_age.groupby('Pclass').median()
print (medians)

for i in range (3):
    idx = np.where( (combined_pd['Pclass']==i+1) & (combined_pd['Age'].isnull()))[0]
    combined_pd.loc[idx,'Age'] = medians.values [1][0]

display (combined_pd)

display(combined_pd[combined_pd[ 'Fare' ].isnull()])

sim_fares = combined_pd[ (combined_pd[ 'Pclass'] == 3) & (combined_pd['Embarked'] == 'S') ]['Fare']
print('Median:', sim_fares.median())
plt.hist (sim_fares)
plt.plot()

combined_pd[ 'Fare']. fillna(sim_fares.median(), inplace=True)

display(combined_pd[combined_pd['Embarked'].isnull()])

sim_emb = combined_pd[ (combined_pd['Pclass'] == 1) & (combined_pd[ 'Fare'] >= 70) & (combined_pd[ 'Fare'] <= 90)]['Embarked' ]
print (sim_emb. value_counts())
combined_pd[ 'Embarked']. fillna('C', inplace=True)

combined_pd['Cabin'].fillna('M', inplace=True)
combined_pd['Cabin'] = combined_pd[ 'Cabin'].str[0]

print (combined_pd['Cabin'].value_counts())

idx = np.where(combined_pd['Cabin']=='T')[0]
combined_pd.loc[idx, 'Cabin'] = 'M'

display(combined_pd)

print (combined_pd.isnull().sum())

print (combined_pd.nunique() )

print (combined_pd[ 'Pclass']. unique())

print(combined_pd['Name'].unique())
names = combined_pd[ 'Name']
last_names = []
titles = []
first_names = []

for name in names:
    if ','not in name:
        last_names.append ('')
    else:
      last, name = name.split(',',1)
      last_names.append (last)

    if ', 'not in name:
      titles.append('')
    else:
        title, first = name.split('. ', 1)
        titles.append (title)
        first_names.append (first)

last_names = np.array (last_names)
titles = np.array(titles)
first_names = np.array(first_names)

idx = np.where(np.isin(titles, ['Capt', 'Col','Major']))
titles[idx] = 'Military'

idx = np.where(np.isin(titles, ['Don', 'Dona', 'Jonkheer', 'Lady', 'Sir', 'Master', 'the Countess']))
titles[idx] = 'Nobility'

idx = np.where(np.isin(titles, ['Miss', 'Mile', 'Ms']))
titles[idx] = 'Ms'

idx = np.where(np.isin(titles, ['Mrs', 'Mme']))
titles [idx] = 'Mrs'

print('\n')
print(np.unique(titles, return_counts=True))

combined_pd['Title'] = titles

display(combined_pd)

print(combined_pd[ 'Age'].min(), combined_pd['Age'].max)

bins = np.array ([0,10,20,30,40,50,60,70,80])

combined_pd ['Age_Bin'] = pd.cut(combined_pd['Age'], bins)
display (combined_pd)

ticket_dict = dict(combined_pd[ 'Ticket' ].value_counts())

combined_pd['tkt_count'] = combined_pd['Ticket'].map(ticket_dict)

display (combined_pd)

combined_pd['Fare_per_Ticket'] = combined_pd[ 'Fare']/combined_pd['tkt_count']

plt. bar(combined_pd['Fare_per_Ticket'].value_counts().index.values, combined_pd['Fare_per_Ticket'].value_counts())
plt.show()

print(combined_pd['Fare_per_Ticket'].min(), combined_pd['Fare_per_Ticket'].max())

bins = [0,20,40,60,80,150]
combined_pd['Fare_Bin'] = pd.cut(combined_pd['Fare_per_Ticket'], bins)
display(combined_pd)

combined_pd[ 'Num_Family'] = combined_pd['SibSp'] + combined_pd['Parch'] + 1

display(combined_pd)

combined_pd.drop(['Name', 'Age', 'Ticket', 'Fare', 'tkt_count', 'Fare_per_Ticket'], axis=1, inplace=True)

display(combined_pd)

print(combined_pd.nunique())

display(combined_pd)

!pip install scikit-learn

from sklearn.model_selection import GridSearchCV

from sklearn.datasets import load_iris
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import MinMaxScaler

combined_pd.set_index('PassengerId', inplace=True)
display(combined_pd)

from sklearn.preprocessing import LabelEncoder
label_enc = combined_pd. copy ()
label_enc = label_enc.astype(str)

label_enc = label_enc.apply(LabelEncoder().fit_transform)

display(label_enc)

one_hot = label_enc. copy()

one_hot = pd. get_dummies(one_hot, columns=['Sex', 'Embarked', 'Title'])

display(one_hot)

X = one_hot. loc[train_idx]. values
y = survived. values

scaler = MinMaxScaler ()
scaler. fit (X)
X_scaled = scaler. fit_transform (X)

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=.2, random_state=0)

print(X_train. shape, X_test. shape, y_train.shape, y_test. shape)

clf = RandomForestClassifier (random_state=0)
clf. fit(X_train, y_train).score(X_test, y_test)

params = { 'criterion': ['gini', 'entropy'],
          'n_estimators': [20, 50, 100, 200, 300, 400, 500, 800, 1000],
          'max_depth' :np.arange (3,50),
          'min_samples_split':np.arange(1,1000),
          'max_features': ['sart', 'log2'],
          'max_samples' : np.linspace(0.1,0.9,10)}

rand_search = RandomizedSearchCV(RandomForestClassifier(random_state=0), params, scoring='accuracy', random_state=0, cv=5)
rand_search.fit(X_train, y_train)
rand_params = rand_search.best_params_
print(rand_params, '\n')
print( 'Train Acc:', rand_search.best_score_)
preds = rand_search.predict(X_test)
print('Test Ace:', accuracy_score(preds, y_test))

n_estimators = np.linspace(rand_params['n_estimators']-10, rand_params[ 'n_estimators']+10, 3).astype(int)

min_samples_split = np.arange(rand_params['min_samples_split']-3, rand_params['min_samples_split']+3).astype(int)

max_samples = np.linspace(rand_params['max_samples']-.05, rand_params ['max_samples']+.05, 6)

max_depth = np.arange(rand_params['max_depth']-5, rand_params ['max_depth']+5).astype (int)

params = { 'criterion': [rand_params[ 'criterion' ]],
          'n_estimators': n_estimators,
           'max_depth': max_depth,
           'min_samples_split':min_samples_split,
           'max_features' : [rand_params ['max_features']],
           'max_samples': max_samples }

grid_search = GridSearchCV(RandomForestClassifier(random_state=0), params, scoring='accuracy', cv=5)
grid_search.fit(X_train, y_train)

grid_params = grid_search.best_params_
print(grid_params, '\n')
print( 'Train Acc:', grid_search.best_score_)
preds = grid_search.predict(X_test)
print('Test Acc:', accuracy_score(preds, y_test))

x = one_hot. loc[train_idx].values
y = survived. values
scaler = MinMaxScaler()
scaler. fit(X)
X_scaled = scaler. fit_transform(X)
X_train, X_test, Y_train, y_test = train_test_split(X_scaled, y, test_size=.2, random_state=0)
print(X_train.shape, X_test.shape, Y_train.shape, y_test.shape)

from sklearn.ensemble import AdaBoostClassifier

clf = AdaBoostClassifier (random_state=0)
clf.fit(X_train, y_train).score(X_test, y_test)

params = {'n_estimators': [50,100,200,300,400,500,800,100],
          'learning_rate':[1.0, 0.1, 0.01, 0.001]}

rand_search = RandomizedSearchCV(AdaBoostClassifier(random_state=0), params, scoring='accuracy', random_state=0, cv=5)
rand_search.fit(X_train, y_train)

rand_params = rand_search.best_params_
print(rand_params, '\n')
print('Train Acc:', rand_search.best_score_)
preds = rand_search.predict(X_test)
print('Test Acc:', accuracy_score(preds, y_test))

n_estimators = np. linspace(rand_params['n_estimators']-50, rand_params['n_estimators']+50, 3).astype(int)

learning_rate = np. linspace(rand_params ['learning_rate']*.9, rand_params ['learning_rate']*1.1, 3)

params = { 'n_estimators': n_estimators,
          'learning_rate': learning_rate}
grid_search = GridSearchCV(AdaBoostClassifier(random_state=0), params, scoring='accuracy', cv=5)
grid_search.fit(X_train, y_train)

grid_params = grid_search.best_params_
print(grid_params, '\n')
print('Train Acc:',grid_search.best_score_)
preds = grid_search.predict(X_test)
print('Test Acc:', accuracy_score(preds, y_test))